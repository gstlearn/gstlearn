{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7718c2a8",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88fcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa49d3",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf0a13",
   "metadata": {},
   "source": [
    "In this preamble, we load the **gstlearn** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a5aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gstlearn as gl\n",
    "import gstlearn.plot as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "ndim = 2\n",
    "gl.defineDefaultSpace(gl.ESpaceType.RN, ndim)\n",
    "gl.OptCst.define(gl.ECst.NTCAR, 15)\n",
    "gp.setDefaultGeographic(dims=[8,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e674b30",
   "metadata": {},
   "source": [
    "We load the data bases containing the observations (`dat`) and the target points (`target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53f8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://soft.minesparis.psl.eu/gstlearn/data/Scotland/Scotland_Temperatures.NF'\n",
    "temp_nf, head = urllib.request.urlretrieve(url)\n",
    "dat = gl.Db.createFromNF(temp_nf)\n",
    "dat.setName(\"*temp\", \"Temperature\")\n",
    "url = 'https://soft.minesparis.psl.eu/gstlearn/data/Scotland/Scotland_Elevations.NF'\n",
    "elev_nf, head = urllib.request.urlretrieve(url)\n",
    "target = gl.DbGrid.createFromNF(elev_nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7515ded",
   "metadata": {},
   "source": [
    "The `target` data base a (grid) map of the elevation across Scotland.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2b9be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb46be",
   "metadata": {},
   "source": [
    "Let us plot its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd8757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = target.plot(\"Elevation\", flagLegendRaster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00305ae",
   "metadata": {},
   "source": [
    "The `dat` data base contains 236 (point) samples of two variables across Scotland: elevation and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d71f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b2de1",
   "metadata": {},
   "source": [
    "We can use the `dbStatisticsPrint` function to compute statistics on variables of a `Db`. We specify \n",
    "\n",
    "* the `Db` of interest (argument `db`),\n",
    "* a vector containing the names of the variables of interest (argument `names`),\n",
    "* the information we want to have on the variable (argument `oper`). This last argument is set through a `EStatOption` object (run `EStatOption_printAll()` for the full list)\n",
    "* a flag `flagMono` allowing to specify whether we want to compute the statistics for each variable separately (`flagMono=TRUE`), or whether we want to compute \"filtered\" statistics (`flagMono=FALSE`). In the latter case, the following table is returned: the entry at the i-th row and j-th column contains the statistics of the i-th variable in `names`, computed for the points where `j`-th variable is defined \n",
    "\n",
    "For instance, let us count the number of observations of each variable using the `dbStatisticsPrint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb5b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(db=dat, names=[\"Elevation\", \"Temperature\"],\n",
    "                     opers=[gl.EStatOption.NUM],flagIso = False,\n",
    "                     title=\"Number of observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d46b0",
   "metadata": {},
   "source": [
    "Since the `Db` `dat` contains 236 samples, we can conclude that the `Elevation` variable is defined at every point, but not the `Temperature` variable. Similarly, we can compute the mean of each variable in the observation data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(db=dat, names=[\"Elevation\", \"Temperature\"],\n",
    "                     opers=[gl.EStatOption.MEAN],flagIso = False,\n",
    "                     title=\"Mean of observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3fe42a",
   "metadata": {},
   "source": [
    "Finally, we can compute the mean elevation `target` data base to compare it to the one in the observation data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a790aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(db=target, names=[\"Elevation\"],\n",
    "                     opers=[gl.EStatOption.MEAN],flagIso = True,\n",
    "                     title=\"Mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9abe3",
   "metadata": {},
   "source": [
    "We can then compute the filtered means for the `Elevation` and `Temperature` variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcbf16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab = gl.dbStatisticsMultiT(db=dat, names=[\"Elevation\", \"Temperature\"],\n",
    "                         oper=gl.EStatOption.MEAN,flagMono = False)\n",
    "tab.setTitle(\"Mean of observations\")\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84466ede",
   "metadata": {},
   "source": [
    "As explained above, the first row of the table contains contains means of the `Elevation` variable. The first one corresponds to the  mean of the `Elevation` variable over all the locations where it is defined, and the second one corresponds to the mean of the `Elevation` variable over all the location where the `Temperature` variable is defined. Hence, we see that the points where the temperature is observed have an average elevation (87.97351)  that is significantly lower than the average elevation in Scotland (241.152), meaning that they are located at relatively low altitudes within the Scottish landscape. \n",
    "\n",
    "To confirm that, we plot the points where the temperature is sampled on top of the elevation map of Scotland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967ec6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = target.plot(name_raster=\"Elevation\")\n",
    "ax = dat.plot(name_size=\"Temperature\", color=\"yellow\", sizmin=0.1, sizmax=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeedbb2",
   "metadata": {},
   "source": [
    "From observing this last plot, it seems like the bigger points (corresponding to high temperatures) are located where the elevation is smaller: this seems to hint (confirm?) that the temperature is negatively correlated with the elevation. To corroborate this, we plot a correlation plot between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb54aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = gp.correlation(dat, namex=\"Elevation\", namey=\"Temperature\", asPoint=True, regrLine=True)\n",
    "ax.decoration(title=\"Correlation between Temperature and Elevation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d8e80",
   "metadata": {},
   "source": [
    "To end this preamble, we create a \"neighborhood\" object specifying a unique neighborhood, which we will use throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e47d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueNeigh = gl.NeighUnique.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce670aa4",
   "metadata": {},
   "source": [
    "## Working with polynomial trends (intrinsic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e8c69",
   "metadata": {},
   "source": [
    "### Fitting a model with a polynomial trend "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc930895",
   "metadata": {},
   "source": [
    "To illustrate our purpose, let us fit a model onto the temperature observations in the `dat` data base. To do so, we start by computing an experimental directional variogram `vario_raw2dir` of the \"raw\" temperature observations, along two directions ($0^{\\circ}\\text{C}$ and $90^{\\circ}\\text{C}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8b251",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "varioparam = gl.VarioParam.createMultiple(ndir=2, npas=30, dpas=10)\n",
    "vario_raw2dir = gl.Vario(varioparam, dat)\n",
    "err = vario_raw2dir.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc912ed",
   "metadata": {},
   "source": [
    "When plotting the experimental variograms, we notice that in one of the directions, the variogram does not reach a plateau. This suggests that a non-stationary model could be more suited for the analysis of these temperature observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f44269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(vario_raw2dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec04cb",
   "metadata": {},
   "source": [
    "Hence, we move to a model for the data where the temperature $Z$ at each location is considered to have a polynomial trend, i.e. we can write, for any location $x$,\n",
    "$$ Z(x) = P(x) + \\varepsilon(x)$$\n",
    "where $P$ is a polynomial function of the coordinates $x$, and $\\varepsilon$ denotes stationary residuals. This model can also be referred to as an intrinsic model. \n",
    "\n",
    "It is possible to directly compute the experimental variogram of the residuals $\\varepsilon$. To do so, we start by creating a `Model` object in which we specify, through the `setDriftIRF` method, the (maximal) degree of the polynomial trend `P`. \n",
    "\n",
    "Then, computing the (possibly directional) experimental variogram of the residuals is done in the same way as for an \"ordinary\" experimental variogram, the only difference being that we now add in the `compute` method of the `Vario` class, the `Model` object specifying the polynomial trend (argument `model`). Then, the methods takes care of filtering out of the observations a polynomial trend of the specified degree, before computing the experimental variogram on the remaining residuals.\n",
    "\n",
    "For instance, if we want to compute an experimental variogram of the residuals from a model with a linear trend (polynomial of degree 1), then we can run the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8db2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model and set polynomial drift of degree 1\n",
    "polDriftModel = gl.Model.create()\n",
    "err = polDriftModel.setDriftIRF(1)\n",
    "\n",
    "# Compute variogram of residuals\n",
    "vario_res2dir = gl.Vario(varioparam, dat)\n",
    "err = vario_res2dir.compute(model=polDriftModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271e421",
   "metadata": {},
   "source": [
    "If we now plot the experimental variograms computed on the raw data (dotted lines) and on the residuals from the intrinsic model (dashed line), we see that in the latter case, we now reach a plateau in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36490cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(vario_raw2dir, linestyle='dotted')\n",
    "ax = gp.varmod(vario_res2dir, linestyle='dashed')\n",
    "ax.decoration(title=\"Temperature (°C)\", xlabel = \"Distance (km)\", ylabel = \"Variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b0b3d",
   "metadata": {},
   "source": [
    "Finally, to compute the coefficients of the polynomial trend from the intrinsic model defined above, we can use the function `regression`. This function allows to perform linear regressions on variables defined in a data base. \n",
    "\n",
    "For instance, to fit a linear trend on the temperature observations in the `dat` data base, we specify the name of response variable (argument `name0`) and the drift model (argument `model`), and set the argument `mode=2` to specify that we would like to compute a regression defined from a drift model. Finally we set the argument `verbose=TRUE` to print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb61ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.regression(dat, name0=\"Temperature\", mode=2, model=polDriftModel, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdb241",
   "metadata": {},
   "source": [
    "Note: The regression results are stored in an object. We can access the coefficients through the slot `coeffs`, the initial variance of the data through the slot `variance` and the variance of the residuals through the slot `varres`.\n",
    "\n",
    "Going back to our example, it is now time to fit our model with polynomial drift. We can now fit that model on the experimental variorgram using the `fit` method in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ce161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = polDriftModel.fit(vario_res2dir,types=[gl.ECov.NUGGET, gl.ECov.EXPONENTIAL, gl.ECov.CUBIC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a5796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(vario_res2dir, polDriftModel)\n",
    "ax.decoration(title=\"Experimental and fitted variogram models - Residuals\", \n",
    "              xlabel = \"Distance (km)\", ylabel = \"Variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e4922",
   "metadata": {},
   "source": [
    "### Universal Kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3f866",
   "metadata": {},
   "source": [
    "To work with universal kriging, it suffices to call the `kriging` (to compute predictions) or `xvalid` (to perform cross-validation) with a `Model` object that includes polynomial drift functions. This model should be fitted on a experimental variogram computed on residuals where the polynomial trend has been filtered out. \n",
    "\n",
    "To perform cross-validation using Universal Kriging, we simply call the `xvalid` function with the model we just fitted on the residuals. Note: we specify the `namconv` argument so that the cross-validation outputs are added to the data base with a name starting with \"CV_UK\", and without changing the locators in the data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93467fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.xvalid(dat, model=polDriftModel, neigh=uniqueNeigh, \n",
    "                namconv=gl.NamingConvention.create(prefix=\"CV_UK\",flag_locator=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbee1d3",
   "metadata": {},
   "source": [
    "The mean Mean Squared cross-validation and standardized errors of the universal kriging predictor are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785bf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_UK*esterr*\")))\n",
    "print(\"Mean squared cross-validation error:\", round(mse,3))\n",
    "\n",
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_UK*stderr*\")))\n",
    "print(\"Mean squared standardized error:\", round(mse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab65066",
   "metadata": {},
   "source": [
    "Finally, we perform an universal kriging prediction of the temperature on the `target` grid using the model fitted on the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f78e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.kriging(dbin=dat, dbout=target, model=polDriftModel, \n",
    "              neigh=uniqueNeigh,\n",
    "              namconv=gl.NamingConvention.create(prefix=\"UK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bbf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = target.plot(name_raster=\"UK*estim\")\n",
    "ax = dat.plot(flagCst=True, color=\"black\")\n",
    "ax.decoration(title=\"Temperature - Universal Kriging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ce320",
   "metadata": {},
   "source": [
    "We compute some statistics on the predicted values using the `dbStatisticsMonoT` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422011a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opers = gl.EStatOption.fromKeys([\"NUM\",\"MINI\",\"MAXI\",\"MEAN\",\"STDV\"])\n",
    "\n",
    "gl.dbStatisticsPrint(target, names = ([\"UK.T*\"]), opers=opers,\n",
    "                    title=\"Statistics on the Universal Kriging:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eadf42",
   "metadata": {},
   "source": [
    "### Comparison with ordinary kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8d586",
   "metadata": {},
   "source": [
    "Let us compare the results obtained when considering a polynomial trend in the model, with the case where where work directly on the \"raw\" temperature observations.\n",
    "\n",
    "To do so, we start by fitting a model on the experimental variogram of raw temperature observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef206ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitmod_raw = gl.Model()\n",
    "err = fitmod_raw.fit(vario_raw2dir,\n",
    "                    types=[gl.ECov.NUGGET, gl.ECov.EXPONENTIAL, gl.ECov.CUBIC, gl.ECov.LINEAR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28613e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(vario_raw2dir, fitmod_raw)\n",
    "ax.decoration(title=\"Experimental and fitted variogram models - Raw temperature observations\", \n",
    "              xlabel = \"Distance (km)\", ylabel = \"Variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bd9cb",
   "metadata": {},
   "source": [
    "We then perform a cross-validation of the fitted model using Ordinary Kriging, and calculate the Mean Squared cross-validation and standardized errors. Since we want to use ordinary kriging, we add a constant drift to the model before calling the `xvalid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6860b1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Add constant drift to model\n",
    "err = fitmod_raw.addDrift(gl.Drift1())\n",
    "\n",
    "## Compute cross-validation\n",
    "err = gl.xvalid(dat, model=fitmod_raw, \n",
    "             neigh=uniqueNeigh,\n",
    "             namconv=gl.NamingConvention.create(prefix=\"CV_OK\",flag_locator=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53e0c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_OK*esterr*\")))\n",
    "print(\"Mean squared cross-validation error:\", round(mse,3))\n",
    "\n",
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_OK*stderr*\")))\n",
    "print(\"Mean squared standardized error:\", round(mse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7124d",
   "metadata": {},
   "source": [
    "We observe that the mean squared errors that we obtain are larger than the one resulting from the intrinsic model. \n",
    "\n",
    "We now perform an ordinary kriging prediction of the temperature on the `target` grid using the model fitted on the raw observations, compute statistics on the predicted values using the `dbStatisticsMonoT` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a74aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.kriging(dbin=dat, dbout=target, model=fitmod_raw, \n",
    "              neigh=uniqueNeigh,\n",
    "              namconv=gl.NamingConvention.create(prefix=\"OK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22843cc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = target.plot(name_raster=\"OK*estim\")\n",
    "ax = dat.plot(flagCst=True, color=\"black\")\n",
    "ax.decoration(title=\"Temperature - Ordinary Kriging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d975b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"OK.T*\"]), opers=opers,\n",
    "                    title=\"Statistics on the Ordinary Kriging:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785965c",
   "metadata": {},
   "source": [
    "Finally, we plot a correlation plot between the ordinary and universal kriging predictions, compare their respective statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220fece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = gp.correlation(target, namex=\"OK*estim\", namey=\"UK*estim\", bissLine=True, bins=100,cmin=1)\n",
    "ax.decoration(xlabel=\"Ordinary Kriging\",ylabel=\"Universal Kriging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838a6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"OK.T*estim\", \"UK.T*estim\"]), opers=opers,\n",
    "                    title=\"Comparison between Ordinary and Universal kriging predictions:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bad12",
   "metadata": {},
   "source": [
    "## Multivariate Models and Cokriging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff57069",
   "metadata": {},
   "source": [
    "To create and work with multivariate models, we simply need to work with `Db` objects containing more than one variable with a `z` locator. All the variables with a `z` locator will be considered as part of the multivariate model. Then, the same commands as in the monovariate case can be used to create and fit experimental variograms, and to perform (co)kriging predictions. \n",
    "\n",
    "Let us illustrate our point with our running example. We would like now to consider a bivariate model of the temperature and the elevation. To do so, we simply allocate, in the observation data base `dat`, a `z` locator to both variables using the $setLocators$  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a5b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.setLocators(names=[\"Temperature\", \"Elevation\"], locatorType=gl.ELoc.Z)\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e00bb",
   "metadata": {},
   "source": [
    "### Fitting a bivariate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91793140",
   "metadata": {},
   "source": [
    "To create experimental (directional) variograms and cross-variograms, we use the same commands as in the monovariate case: since the data base `dat` now contains two variables with a `z` locator, the `compute` method automatically computes both variograms and cross-variograms for these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3742e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "varioexp2var = gl.Vario.create(varioparam, dat)\n",
    "err = varioexp2var.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b43012",
   "metadata": {},
   "source": [
    "We can then plot the experimental variograms and cross-variograms with a simple command: the plot in the i-th row and j-th column corresponds to the cross-variogram between the variables with locators `zi` and `zj` (hence the diagonal plots correspond to the variograms of each variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb26534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=gp.varmod(varioexp2var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568e11c",
   "metadata": {},
   "source": [
    "To fit a model on the experimental variograms and cross-variograms, we use the same commands as in the monovariate case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0571eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitmod2var = gl.Model()\n",
    "err = fitmod2var.fit(varioexp2var,\n",
    "                     types=[gl.ECov.NUGGET, gl.ECov.EXPONENTIAL, gl.ECov.CUBIC, gl.ECov.LINEAR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec458d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(varioexp2var, fitmod2var, lw=2)\n",
    "gp.decoration(ax,title=\"Temperature (°C) and Elevation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d16400",
   "metadata": {},
   "source": [
    "### Cokriging predictions and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59e755",
   "metadata": {},
   "source": [
    "Since cokriging can be time-consuming in Unique Neighborhood, we create a small moving neighborhood for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734f4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movingNeigh = gl.NeighMoving.create(radius = 1000, nmaxi = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b71f6",
   "metadata": {},
   "source": [
    "To perform a cross-validation of the bivariate model using co-kriging, we use the  same commands as in the monovariate case. Then cross-validation errors are computed for each variable of the multivariate model (hence for both the Temperature and the Elevation in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57494f39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "err = gl.xvalid(dat, model=fitmod2var,\n",
    "             neigh=movingNeigh,\n",
    "             namconv=gl.NamingConvention.create(prefix=\"CV_COK\",flag_locator=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396f11c",
   "metadata": {},
   "source": [
    "We obtain the following Mean Squared Errors for the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4a118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_COK.Temperature*esterr*\")))\n",
    "print(\"Mean squared cross-validation error:\", round(mse,3))\n",
    "\n",
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_COK.Temperature*stderr*\")))\n",
    "print(\"Mean squared standardized error:\", round(mse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e71d8",
   "metadata": {},
   "source": [
    "We obtain the following Mean Squared Errors for the elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee9190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_COK.Elevation*esterr*\")))\n",
    "print(\"Mean squared cross-validation error:\", round(mse,3))\n",
    "\n",
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_COK.Elevation*stderr*\")))\n",
    "print(\"Mean squared standardized error:\", round(mse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584504d",
   "metadata": {},
   "source": [
    "Similarly, to compute cokriging predictions on the grid, we use the same syntax as in monovariate case: once again, a predictor for each variable in the multivariate model is produced. (Note: we revert back to a unique neighborhood to compare with the predictors previously introduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f9f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.kriging(dbin=dat, dbout=target, model=fitmod2var, \n",
    "              neigh=uniqueNeigh,\n",
    "              namconv=gl.NamingConvention.create(prefix=\"COK\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47c71f",
   "metadata": {},
   "source": [
    "We can then represent the cokriging predictor for the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f37e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = target.plot(name_raster=\"COK.Temp*estim\")\n",
    "ax = dat.plot(flagCst=True, color=\"black\")\n",
    "ax.decoration(title=\"Temperature - CoKriging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56885f97",
   "metadata": {},
   "source": [
    "For this predictor, we get the following statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe169b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"COK.T*\"]), opers=opers,\n",
    "                    title=\"Statistics on the CoKriging predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f9fb0",
   "metadata": {},
   "source": [
    "Finally, we compare the cokriging predictor to the ordinary kriging predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266edb2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.correlation(target, namex=\"OK.T*estim\", namey=\"COK.T*estim\", bissLine=True, bins=100, cmin=1)\n",
    "ax.decoration(xlabel=\"Ordinary Kriging\",ylabel=\"Ordinary CoKriging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53893075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"OK.T*estim\", \"COK.T*estim\"]), opers=opers,\n",
    "                    title=\"Comparison between Ordinary and Universal kriging predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453a330",
   "metadata": {},
   "source": [
    "## Working with residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d8511",
   "metadata": {},
   "source": [
    "In this section, we assume that the variable of interest $Z$ is modeled (at each location $x$) as\n",
    "$$ Z(x) = b+a Y(x) + \\varepsilon(x)$$\n",
    "where $Y$ is an auxiliary variable known at every location, $a$ and $b$ are some (unknown) regression coefficients, and $\\varepsilon$ denotes stationary residuals. Our goal will be to model and work directly with the residuals $\\varepsilon$ (since they are the one who are assumed to be stationary).\n",
    "\n",
    "In our running example, the variable of interest $Z$ will be the temperature, and the auxiliary variable $Y$ will be the elevation. To compute the coefficients $a$ and $b$ of the linear regression between the temperature and the elevation, we can use once again the `regression` function. We specify the name of response variable (argument `name0`) and the names of the auxiliary variables (argument `names`), and set the argument `mode=0` to specify that we would like to compute a regression defined from the variables with the specified names. We also set the argument `flagCste=TRUE` to specify that we are looking for an affine regression model between the variables (i.e. that includes the bias coefficient `b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf6d62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = gl.regression(dat, \"Temperature\", [\"Elevation\"], mode=0, flagCste=True)\n",
    "\n",
    "#Extract coefficients\n",
    "b = regr.coeffs[0]\n",
    "a = regr.coeffs[1]\n",
    "\n",
    "print(\"Coefficients: b=\",round(b,3),\"; a=\",round(a,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938197f",
   "metadata": {},
   "source": [
    "From these regression coefficients obtained above, we can then compute the residuals explicitly as $ \\varepsilon(x)=Z(x) - (b+a Y(x) )$. An alternative method consists in using the `dbRegression` function: this functions fits a regression model, computes the residuals and add them directly on the data base containing the data. The `dbRegression` function is called in a similar way as the `regression` function.\n",
    "\n",
    "In the next example, we compute the residuals of the linear regression between the temperature and the elevation and add them to the observation data base (with a name starting with \"RegRes\"and without changing the locators in the data base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce36e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.dbRegression(dat, \"Temperature\", [\"Elevation\"], flagCste=True,\n",
    "                     namconv = gl.NamingConvention.create(\"RegRes\",flag_locator=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f90a4",
   "metadata": {},
   "source": [
    "We then compute some statistics about these residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd62eda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(dat, names = ([\"RegRes*\"]), opers=opers,\n",
    "                    title=\"Statistics on the residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840a632",
   "metadata": {},
   "source": [
    "Finally we plot a correlation plot between the residuals and the regressor variable (i.e. the elevation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdd625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.correlation(dat, namex=\"Elevation\", namey=\"RegRes*\",regrLine=True,asPoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7935042",
   "metadata": {},
   "source": [
    "Now that the residuals are explicitly computed and available in the observation data base, we can proceed to work with them as with any other variable.\n",
    "\n",
    "We start by setting their locator to `z` to specify that they are now our variable of interest within the data base (instead of the raw temperature observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35754de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.setLocator(\"RegRes*\",gl.ELoc.Z, cleanSameLocator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced47b0",
   "metadata": {},
   "source": [
    "Then we can compute an experimental variogram for the residuals and fit a model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545dc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute experimental variogram\n",
    "varioexpR = gl.Vario(varioparam, dat)\n",
    "err = varioexpR.compute()\n",
    "\n",
    "## Fit model\n",
    "fitmodR = gl.Model()\n",
    "err = fitmodR.fit(varioexpR,types=[gl.ECov.NUGGET, gl.ECov.SPHERICAL, gl.ECov.LINEAR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(varioexpR, fitmodR)\n",
    "ax.decoration(title=\"Experimental and fitted variogram models - Temperature Residual\",\n",
    "              xlabel = \"Distance (km)\", ylabel = \"Variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fef3d9",
   "metadata": {},
   "source": [
    "Finally, we can compute an kriging prediction of the residuals and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae90ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  Add constant drift to model to perform ordinary kriging\n",
    "err = fitmodR.addDrift(gl.Drift1())\n",
    "\n",
    "## Compute kriging\n",
    "err = gl.kriging(dbin=dat, dbout=target, model=fitmodR, \n",
    "              neigh=uniqueNeigh,\n",
    "              namconv=gl.NamingConvention.create(prefix=\"ROK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2583d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = target.plot(\"ROK*estim\")\n",
    "ax = dat.plot(flagCst=True, color=\"black\")\n",
    "ax.decoration(title=\"Temperature residuals - Ordinary Kriging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced43bb",
   "metadata": {},
   "source": [
    "Now that the residuals $\\varepsilon^{OK}$ are predicted everywhere on the grid (by ordinary kriging), we can compute a predictor $Z^*$ for the temperature by simply adding the back the linear regression part of the model, i.e. by computing\n",
    "\n",
    "$$\n",
    "Z^{\\star}(x) = b + a Y(x) + \\varepsilon^{OK}(x)\n",
    "$$\n",
    "We can compute this predictor by directly manipulating the variables of the `target` data base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafa0b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Compute temperature predictor\n",
    "ROK_estim =  b + a * target[\"Elevation\"] + target[\"ROK*estim\"]\n",
    "\n",
    "## Add it to data base\n",
    "uid = target.addColumns(ROK_estim,\"KR.Temperature.estim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60ecc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = target.plot(\"KR.T*estim\")\n",
    "ax = dat.plot(flagCst=True, color=\"black\")\n",
    "ax.decoration(title=\"Temperature - Ordinary Kriging of the residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280c1f6",
   "metadata": {},
   "source": [
    "Finally, we compare the predictor obtained by kriging of the residuals to the ordinary kriging predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580f5f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = gp.correlation(target, namex=\"OK.T*estim\", namey=\"KR.T*estim\", bissLine=True, bins=100, \n",
    "                    flagSameAxes=True, cmin=1)\n",
    "ax.decoration(xlabel=\"Ordinary Kriging\",ylabel=\"Ordinary CoKriging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d4e5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"OK.T*estim*\", \"KR.T*estim\"]), opers=opers,\n",
    "                    title=\"Comparison between Ordinary and Residual kriging predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f967a",
   "metadata": {},
   "source": [
    "## Models with External Drifts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e8838",
   "metadata": {},
   "source": [
    "In this last section, we investigate how to specify and work with external drifts when modeling data. In a data base, external drifts are identified by allocating a locator `f` to them in the data bases.\n",
    "\n",
    "For instance, circling back to our running example, let us assume that we would like to model the temperature with an external drift given by the elevation. Then, in the observation data base, we simply need to allocate a `z` locator to the temperature variable and a `f` locator to the elevation variable using the `setLocator` method. Note: we use the flag `cleanSameLocator=TRUE` to make sure that only the variable we specify carries the locator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dabd8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Set `z` locator to temperature\n",
    "dat.setLocator(\"Temperature\",gl.ELoc.Z,cleanSameLocator=True)\n",
    "\n",
    "## Set `f` locator to elevation\n",
    "dat.setLocator(\"Elevation\",gl.ELoc.F,cleanSameLocator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbd344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "varioKED = gl.Vario(varioparam, dat)\n",
    "\n",
    "model = gl.Model.create()\n",
    "model.setDriftIRF(order=0, nfex=1)\n",
    "err = varioKED.compute(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d3e11",
   "metadata": {},
   "source": [
    "Then, defining and fitting the models on the one hand, and performing kriging predictions on the other hand, is done using the same approach as the one described earlier for models with a polynomial trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f809a4",
   "metadata": {},
   "source": [
    "### Fitting  a model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fec2bf",
   "metadata": {},
   "source": [
    "To create experimental (directional) variograms of the residuals from a model with external drift, we use the same approach as the one described for modeling data with polynomial trends.\n",
    "\n",
    "First, we create a `Model` object where we specify that we deal with external drifts. This is once again done through the `setDriftIRF` function where we specify:\n",
    "\n",
    "* the number of external drift variables (argument `nfex`): this is the number of variables with a `f` locator the we want to work with \n",
    "* the maximal degree of the polynomial trend in the data (argument `order`):  setting `order=0` allows to add a constant drift to the model, and setting `order` to a value $n$ allows to add all the monomial functions of the coordinates of order up to $n$ as external drifts (on top of the `nfex` external drift variables)\n",
    "\n",
    "Circling back to our example, we create a model with a single external drift (the elevation), and add a constant drift (that basically acts like a bias term). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a4e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EDmodel = gl.Model()\n",
    "EDmodel.setDriftIRF(order = 0, nfex = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc49f4",
   "metadata": {},
   "source": [
    "Then, to compute the experimental variogram, we use the same commands as in the case of polynomial trends: we create a `Vario` object from the data base containing the data, and call the `compute` method with the model we just created. The experimental variogram is computed on the residuals obtained after \"filtering out\" the (linear) effect of the drift  variables (and possibly of a polynomial trend if specified in the model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "varioKED = gl.Vario(varioparam, dat)\n",
    "err = varioKED.compute(model=EDmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb2a10",
   "metadata": {},
   "source": [
    "As a reference, we plot the experimental variograms computed on the raw temperature data (dotted lines) and on the residuals from the model with external drift (dashed line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a849df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(vario_raw2dir, linestyle='dotted')\n",
    "ax = gp.varmod(varioKED, linestyle='dashed')\n",
    "ax.decoration(title=\"Temperature (°C)\",\n",
    "              xlabel = \"Distance (km)\", ylabel = \"Variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16673cb",
   "metadata": {},
   "source": [
    "Finally, we fit our model with external drift using the `fit` method (which we call on the experimental variogram of residuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = EDmodel.fit(varioKED,types=[gl.ECov.NUGGET, gl.ECov.CUBIC, gl.ECov.GAUSSIAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337f8ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = gp.varmod(varioKED, EDmodel)\n",
    "ax.decoration(title=\"Experimental and fitted variogram models - Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fe8cb",
   "metadata": {},
   "source": [
    "### Cross-Validation and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd25a8e",
   "metadata": {},
   "source": [
    "To perform a cross-validation or predictions using kriging with External Drifts, we simply call the `xvalid` and `kriging` functions with models where external drifts are specified. \n",
    "\n",
    "For instance, in our example, a cross-validation is performed by calling the `xvalid` function with the model we just fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ed802",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "err = gl.xvalid(dat, model=EDmodel, \n",
    "             neigh=uniqueNeigh,\n",
    "             namconv=gl.NamingConvention.create(prefix=\"CV_KED\",flag_locator=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b95514",
   "metadata": {},
   "source": [
    "The mean Mean Squared cross-validation and standardized errors of the resulting kriging predictor are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb167be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_KED*esterr*\")))\n",
    "print(\"Mean squared cross-validation error:\", round(mse,3))\n",
    "\n",
    "mse=np.nanmean(np.square(dat.getColumn(\"CV_KED*stderr*\")))\n",
    "print(\"Mean squared standardized error:\", round(mse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1bfbb",
   "metadata": {},
   "source": [
    "Finally, we perform an kriging prediction of the temperature on the `target`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0a92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = gl.kriging(dbin=dat, dbout=target, model=EDmodel, \n",
    "              neigh=uniqueNeigh,\n",
    "              namconv=gl.NamingConvention.create(prefix=\"KED\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b8a60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = target.plot(\"KED.T*estim\")\n",
    "ax = dat.plot(flagCst=True, color=\"black\")\n",
    "ax.decoration(title=\"Temperature - Kriging with external drift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75590a41",
   "metadata": {},
   "source": [
    "For this predictor, we get the following statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e80992",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"KED.T*\"]), opers=opers,\n",
    "                    title=\"Statistics on the Kriging with External Drift predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf5907",
   "metadata": {},
   "source": [
    "### Comparing the various kriging predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb4919",
   "metadata": {},
   "source": [
    "First, we create a correlation plot between the ordinary kriging predictions, and the kriging with external drift (KED) predictions. Note that negative Estimates are present when using External Drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34e791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = gp.correlation(target, namex=\"OK.T*estim\", namey=\"KED.T*estim\", bissLine=True, bins=100,\n",
    "                    flagSameAxes=True, cmin=1)\n",
    "ax.decoration(xlabel=\"Ordinary Kriging\",ylabel=\"Kriging with External Drift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45ec3f",
   "metadata": {},
   "source": [
    "We then compare the  Mean Squared cross-validation errors obtained in for the different kriging predictions (UK=Universal kriging, OK=Ordinary kriging, COK= Cokriging, KED= Kriging with external drift, KR=Kriging of residuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e676a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(dat, names = ([\"CV*.Temperature.esterr*\"]), opers=opers,\n",
    "                    title=\"Mean-squared cross-validation errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048ba55",
   "metadata": {},
   "source": [
    "We then compare various statistics computed for each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c53397",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"*.Temperature.estim\"]), opers=opers,\n",
    "                    title=\"Statistics of the predictors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80bcae",
   "metadata": {},
   "source": [
    "Finally, we compare various statistics computed for the standard-deviation of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.dbStatisticsPrint(target, names = ([\"*.Temperature.stdev\"]), opers=opers,\n",
    "                    title=\"Statistics of the standard-deviation of each predictors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "322.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
