{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gstlearn as gl\n",
    "import gstlearn.test as gt\n",
    "import gstlearn.document as gdoc\n",
    "import numpy as np\n",
    "from IPython.display import Markdown\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<style>md-block { color:gray; background-color:white; }</style><md-block>\n",
       "# Kriging \n",
       "\n",
       "Let suppose that \n",
       "\n",
       "$Z(s_i) = X_i\\beta + Y(s_i)$\n",
       "\n",
       "where $Y$ is a second order stationary random field with mean 0 and covariance function $C$.\n",
       "\n",
       "If $Z$ is a vector of observations, we denote \n",
       "$Z = X\\beta + Y$ with $\\Sigma$ the covariance of Y\n",
       "\n",
       "## Simple Kriging \n",
       "If $\\beta$ is known, we can obtain the simple kriging \n",
       "\n",
       "$Z_0^{SK} = X_0\\beta + \\Sigma_0^t\\Sigma^{-1}(Z-X\\beta) = X_0\\beta + \\lambda_{SK}^t(Z-X\\beta)$\n",
       "\n",
       "with:\n",
       "\n",
       "- the simple kriging weights\n",
       "\n",
       "$\\lambda_{SK}=\\Sigma^{-1}\\Sigma_0$ \n",
       "\n",
       "- the variance of the estimator\n",
       "\n",
       "$\\textrm{Var}(Z_0^{SK})=\\lambda_{SK}^t\\Sigma\\lambda_{SK}=\\lambda_{SK}^t\\Sigma_0$\n",
       "\n",
       "- the estimation variance\n",
       "\n",
       "$\\sigma_{SK}^2 = \\textrm{Var}(Z_0-Z_0^{SK}) = \\sigma_0^2-\\Sigma_0^t\\Sigma^{-1}\\Sigma_0=\\sigma_0^2-\\lambda_{SK}^t\\Sigma_0$\n",
       "\n",
       "### In matrix notation:\n",
       "\n",
       "Simple Kriging System\n",
       "\n",
       "$\n",
       "      \\begin{bmatrix}\n",
       "\t\\Sigma\n",
       "      \\end{bmatrix}\n",
       "      \\times\n",
       "      \\begin{bmatrix}\n",
       "\t\\lambda_{SK}\n",
       "      \\end{bmatrix}\n",
       "      =\n",
       "      \\begin{bmatrix}\n",
       "        \\Sigma_0\n",
       "      \\end{bmatrix}\n",
       "$\n",
       "\n",
       "Estimation\n",
       "\n",
       "$    Z_0^{SK} =\n",
       "     \\begin{bmatrix}\n",
       "\tZ\n",
       "     \\end{bmatrix}^t\n",
       "     \\times\n",
       "     \\begin{bmatrix}\n",
       "\t\\lambda_{SK}\n",
       "     \\end{bmatrix}\n",
       "     + m ({ 1 - \\sum{\\lambda_{SK}}} )\n",
       "$\n",
       "\n",
       "Variance of Estimation error\n",
       "\n",
       "$\n",
       "   \\sigma_{SK}^2 = \\sigma_0^2 -\n",
       "   \\begin{bmatrix}\n",
       "     \\lambda_{SK}\n",
       "   \\end{bmatrix}^t\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\Sigma_0\n",
       "   \\end{bmatrix}\n",
       "$\n",
       "\n",
       "Variance of Estimator\n",
       "\n",
       "$\n",
       "   \\textrm{Var}(Z_0^{SK}) =\n",
       "   \\begin{bmatrix}\n",
       "     \\lambda_{SK}\n",
       "   \\end{bmatrix}^t\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\Sigma\n",
       "   \\end{bmatrix}\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\lambda_{SK}\n",
       "   \\end{bmatrix} =\n",
       "   \\begin{bmatrix}\n",
       "     \\lambda_{SK}\n",
       "   \\end{bmatrix}^t\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\Sigma_0\n",
       "   \\end{bmatrix}\n",
       "$\n",
       "\n",
       "## Universal kriging\n",
       "\n",
       "If $\\beta$ is unknown, we can estimate it by \n",
       "\n",
       "$\\hat\\beta =  \\Sigma_c X^t\\Sigma^{-1}Z$ \n",
       "\n",
       "Introducing the notation\n",
       "\n",
       "$\\Sigma_c =  (X^t\\Sigma^{-1}X)^{-1} $\n",
       "\n",
       "then\n",
       "\n",
       "$\\hat\\beta = \\Sigma_c X^t\\Sigma^{-1}Z$ \n",
       "\n",
       "$\\textrm{Var}(\\hat\\beta)=\\Sigma_c$\n",
       "\n",
       "The Universal kriging is obtained by first computing $\\hat\\beta$ and then pluging $\\hat\\beta$  in the simple kriging procedure.\n",
       "\n",
       "$Z^{UK}_0 = X_0\\hat\\beta + \\Sigma_0^t\\Sigma^{-1}(Z-X\\hat\\beta)= \\Sigma_0^t\\Sigma^{-1}Z + (X_0 - \\Sigma_0^t\\Sigma^{-1}X)\\hat\\beta$\n",
       "\n",
       "We can rewrite everything with respect to $Z$\n",
       "\n",
       "$Z^{UK}_0 =  (\\Sigma_0^t\\Sigma^{-1} + (X_0 - \\Sigma_0^t\\Sigma^{-1}X)\\Sigma_c X^t\\Sigma^{-1})Z \\\\\n",
       "=(\\lambda_{SK}^t+(X_0-\\lambda_{SK}^tX) \\Sigma_c X^t\\Sigma^{-1})Z\\\\\n",
       "=\\lambda_{UK}^tZ$ \n",
       "\n",
       "with\n",
       "\n",
       "- the Universal Kriging Weights\n",
       "\n",
       "$\\lambda_{UK}=\\lambda_{SK}+\\Sigma^{-1}X \\Sigma_c(X_0^t-X^t\\lambda_{SK})$\n",
       "\n",
       "- the variance of the estimator is\n",
       "\n",
       "$\\textrm{Var}(Z^{UK}_0) = \\lambda_{UK}^t\\Sigma\\lambda_{UK} \\\\\n",
       "=\\textrm{Var}(Z^{SK}_0) +2\\lambda_{SK}^tX \\Sigma_c \\Sigma_c (X_0^t-X^t\\lambda_{SK})+(X_0-\\lambda_{SK}^tX)\\Sigma_c X^t\\Sigma^{-1}X\\Sigma_c (X_0^t-X^t\\lambda_{SK})\\\\\n",
       "=\\textrm{Var}(Z^{SK}_0) +2\\lambda_{SK}^tX\\Sigma_c (X_0^t-X^t\\lambda_{SK})+(X_0-\\lambda_{SK}^tX)\\Sigma_c (X_0^t-X^t\\lambda_{SK})\\\\\n",
       "=\\textrm{Var}(Z^{SK}_0)+(\\lambda_{SK}^tX+X_0)\\Sigma_c (X_0^t-X^t\\lambda_{SK})\\\\\n",
       "=\\textrm{Var}(Z^{SK}_0)-\\lambda_{SK}^tX\\Sigma_c X^t\\lambda_{SK}+X_0 \\Sigma_c X_0^t$\n",
       "\n",
       "- the estimation variance\n",
       "\n",
       "$\\sigma_{UK}^2 = \\sigma_0^2 - 2\\textrm{Cov}(Z_0,Z^{UK}_0)+ \\textrm{Var}(Z^{UK}_0)\\\\\n",
       "= \\sigma_0^2 -2\\Sigma_0^t\\lambda_{UK}+\\textrm{Var}(Z^{UK}_0)\\\\\n",
       "= \\sigma_0^2 -2\\Sigma_0^t(\\lambda_{SK}+\\Sigma^{-1}X \\Sigma_c(X_0^t-X^t\\lambda_{SK}))+\\textrm{Var}(Z^{SK}_0)-\\lambda_{SK}^tX \\Sigma_c X^t\\lambda_{SK}+X_0 \\Sigma_c X_0^t\\\\\n",
       "=  \\sigma_0^2 -\\Sigma_0^t\\lambda_{SK} -2\\Sigma_0^t\\Sigma^{-1}X \\Sigma_c (X_0^t-X^t\\lambda_{SK})-\\lambda_{SK}^tX \\Sigma_c X^t\\lambda_{SK}+X_0 \\Sigma_c X_0^t\\\\\n",
       "=\\sigma_{SK}^2-2\\lambda_{SK}^tX \\Sigma_c (X_0^t-X^t\\lambda_{SK})-\\lambda_{SK}^tX \\Sigma_c X^t\\lambda_{SK}+X_0 \\Sigma_c X_0^t\\\\\n",
       "=\\sigma_{SK}^2+(X_0-\\lambda_{SK}^tX) \\Sigma_c (X_0^t-X^t\\lambda_{SK})\n",
       "$\n",
       "\n",
       "### In matrix notation:\n",
       "\n",
       "Universal Kriging System\n",
       "\n",
       "$\n",
       "      \\begin{bmatrix}\n",
       "\t\\Sigma & X \\\\\n",
       "         X^t   & 0\n",
       "      \\end{bmatrix}\n",
       "      \\times\n",
       "      \\begin{bmatrix}\n",
       "\t\\lambda_{UK} \\\\\n",
       "\t-\\mu\n",
       "      \\end{bmatrix}\n",
       "      =\n",
       "      \\begin{bmatrix}\n",
       "        \\Sigma_0 \\\\\n",
       "\tX_0^t\n",
       "      \\end{bmatrix}\n",
       "$\n",
       "\n",
       "Estimation\n",
       "\n",
       "$    Z_0^{UK} =\n",
       "     \\begin{bmatrix}\n",
       "\tZ \\\\\n",
       "\t0\n",
       "     \\end{bmatrix}^t\n",
       "     \\times\n",
       "     \\begin{bmatrix}\n",
       "\t\\lambda_{UK} \\\\\n",
       "\t-\\mu\n",
       "     \\end{bmatrix}\n",
       "$\n",
       "\n",
       "Variance of estimation error\n",
       "\n",
       "$\n",
       "   \\sigma_{UK}^2 = \\sigma_0^2 -\n",
       "   \\begin{bmatrix}\n",
       "     \\lambda_{UK} \\\\\n",
       "     -\\mu\n",
       "   \\end{bmatrix}^t\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\Sigma_0 \\\\\n",
       "     X_0^t\n",
       "   \\end{bmatrix}\n",
       "$\n",
       "\n",
       "Variance of estimator\n",
       "\n",
       "$\n",
       "   \\textrm{Var}(Z^{UK}_0) =\n",
       "     \\begin{bmatrix}\n",
       "     \\lambda_{UK}\n",
       "   \\end{bmatrix}^t\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\Sigma\n",
       "   \\end{bmatrix}\n",
       "   \\times\n",
       "   \\begin{bmatrix}\n",
       "     \\lambda_{UK}\n",
       "   \\end{bmatrix}\n",
       "$\n",
       "\n",
       "\n",
       "# Summary\n",
       "\n",
       "## Simple Kriging\n",
       "\n",
       "- the estimator\n",
       "\n",
       "$Z_0^{SK} = X_0\\beta + \\lambda_{SK}^t(Z-X\\beta) =  \\lambda_{SK}^tZ +(X_0 -\\lambda_{SK}^tX)\\beta$\n",
       "\n",
       "where \n",
       "\n",
       "$\\lambda_{SK}=\\Sigma^{-1}\\Sigma_0$ \n",
       "\n",
       "- the variance of the estimator\n",
       "\n",
       "$\\textrm{Var}(Z_0^{SK})=\\lambda_{SK}^t\\Sigma\\lambda_{SK}=\\lambda_{SK}^t\\Sigma_0$\n",
       "\n",
       "- the variance of the estimation error\n",
       "\n",
       "$\\sigma_{SK}^2 = \\textrm{Var}(Z_0-Z_0^{SK}) = \\sigma_0^2-\\Sigma_0^t\\Sigma^{-1}\\Sigma_0=\\sigma_0^2-\\lambda_{SK}^t\\Sigma_0$\n",
       "\n",
       "## Universal Kriging\n",
       "\n",
       "$\\hat\\beta =  \\Sigma_c X^t\\Sigma^{-1}Z$ \n",
       "\n",
       "$\\textrm{Var}(\\hat\\beta)= \\Sigma_c $\n",
       "\n",
       "- the estimator\n",
       "\n",
       "$Z^{UK}_0 =\\lambda^t_{SK}Z + (X_0 - \\lambda_{SK}^tX)\\hat\\beta= \\lambda^t_{UK}Z$\n",
       "\n",
       "$\\lambda_{UK}=\\lambda_{SK}+\\Sigma^{-1}X \\Sigma_c (X_0^t-X^t\\lambda_{SK})$\n",
       "\n",
       "$\\mu_{UK}=\\Sigma_c (X_0 - \\lambda_{SK}^tX)^t$\n",
       "\n",
       "- the variance of the estimator\n",
       "\n",
       "$\\textrm{Var}(Z^{UK}_0) = \\textrm{Var}(Z^{SK}_0)-\\lambda_{SK}^tX\\Sigma_c X^t\\lambda_{SK}+X_0\\Sigma_c X_0^t$\n",
       "\n",
       "- the variance of the estimation error\n",
       "\n",
       "$\\sigma_{UK}^2 \n",
       "=\\sigma_{SK}^2+(X_0-\\lambda_{SK}^tX) \\Sigma_c (X_0^t-X^t\\lambda_{SK})\n",
       "$\n",
       "</md-block>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(gdoc.loadDoc(\"Kriging.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(gdoc.loadDoc(\"Kriging_Bayes.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cova(x,sills=1):\n",
    "    return np.kron(sills,np.exp(-x/2))\n",
    "\n",
    "np.random.seed(1234)\n",
    "A = np.random.normal(size=(3,3))\n",
    "sills = gl.VectorDouble((A@A.T).reshape(1,-1)[0])\n",
    "model = gl.Model.createFromParam(gl.ECov.EXPONENTIAL,range = 2.,flagRange=False,sills=sills)\n",
    "\n",
    "nx = [10,10]\n",
    "\n",
    "def matriceReduce(m,ind):\n",
    "    M = gl.MatrixSquareSymmetric(len(ind))\n",
    "    for i,j in enumerate(ind):\n",
    "        for k in range(j,len(ind)):\n",
    "            M.setValue(i,k,m.getValue(j,ind[k]))\n",
    "    return M\n",
    "\n",
    "def modelReduce(model,var):\n",
    "    ctxt=model.getContext()\n",
    "    ctxt.setNVar(len(var))\n",
    "    modeln = gl.Model.create(ctxt)\n",
    "    covlist = model.getCovAnisoList()\n",
    "    \n",
    "    for i in range(covlist.getCovaNumber()):\n",
    "        cova = covlist.getCova(i)\n",
    "        sills = matriceReduce(cova.getSill(),var)\n",
    "        covar = gl.CovAniso.createAnisotropicMulti(ctxt,\n",
    "                                             cova.getType(),\n",
    "                                             cova.getRanges(),\n",
    "                                             sills,\n",
    "                                             cova.getParam(),\n",
    "                                             cova.getAnisoAngles())\n",
    "        modeln.addCov(covar)\n",
    "    return modeln\n",
    "\n",
    "\n",
    "def createDbIn(ndat,nvar,percent,ndim=2,selDbin=False,measurement_error=False,ndrift = 0,\n",
    "               flag_isotopic=False,seed=1234):\n",
    "    db = gl.Db.create()\n",
    "    np.random.seed(seed)\n",
    "    for i in range(ndim):\n",
    "        db[\"x\" +str(i)] = np.random.uniform(size = ndat)\n",
    "     \n",
    "    db.setLocators([\"x*\"],gl.ELoc.X)\n",
    "        \n",
    "    indIn = np.arange(ndat)\n",
    "    if selDbin:\n",
    "        np.random.shuffle(indIn)\n",
    "        indIn = np.sort(indIn)\n",
    "        indIn = indIn[range(int(ndat/2))]\n",
    "        sel = np.zeros(shape=ndat)\n",
    "        sel[indIn] = 1\n",
    "        db[\"sel\"] = sel\n",
    "        db.setLocator(\"sel\",gl.ELoc.SEL)\n",
    "      \n",
    "    #Creation of an heterotopic data set\n",
    "    indList = [] \n",
    "    for i in range(nvar):\n",
    "        u = np.array([None for i in range(ndat)])\n",
    "        ind = np.arange(ndat)\n",
    "        if not flag_isotopic and nvar>1: \n",
    "            np.random.shuffle(ind)\n",
    "            ind = ind[range(int(ndat*percent[i]))]\n",
    "        ind = np.sort(list(set(ind) & set(indIn)))\n",
    "        indList += [ind]\n",
    "        vect = np.array([None for i in range(ndat)])\n",
    "        vect[ind] = np.random.normal(size = len(ind))\n",
    "        db[\"z\"+str(i)]=vect\n",
    "          \n",
    "    db.setLocators([\"z*\"],gl.ELoc.Z)\n",
    "    \n",
    "    indF = []\n",
    "    \n",
    "    for i in range(nvar):\n",
    "        indF += list(np.array(indList[i]) + ndat * i)\n",
    "    \n",
    "    if measurement_error :\n",
    "        for i in range(nvar):\n",
    "            db[\"err\"+str(i)] = 0.1 * np.random.uniform(size = ndat)\n",
    "            \n",
    "        db.setLocators([\"err*\"],gl.ELoc.V)\n",
    "    \n",
    "    if ndrift>0:\n",
    "        for i in range(ndrift):\n",
    "            db[\"ff\" + str(i)] = np.random.normal(size = ndat)\n",
    "        db.setLocator(\"ff*\",gl.ELoc.F)\n",
    "    \n",
    "    return db,indF\n",
    "\n",
    "\n",
    "def test_covmat(ndat,nx,nvar,percent,model,cova,\n",
    "                 irf=None,drift=False,measurement_error=False,compute_vars = True,\n",
    "                 selDbin = True, selDbout = True,flag_isotopic=True,\n",
    "                 seed=1234,tol=1e-12,eps=1e-3,test = True,verbose=False):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    ndrift = 1 if drift else 0\n",
    "    modeln = modelReduce(model,range(nvar))\n",
    "    #### Create the description of the case #####\n",
    "    casetxt = \"case:\\n\"\n",
    "    \n",
    "    inter = \"\"\n",
    "    if nvar > 1:\n",
    "        inter = \"co-\"\n",
    "        \n",
    "    if irf is None and drift:\n",
    "        return 0\n",
    "    \n",
    "    if irf is None and not drift:\n",
    "        casetxt += \"- simple \"+ inter+ \"kriging\\n\"\n",
    "    else :\n",
    "        if irf is not None :\n",
    "            casetxt += \"- KU with drift of degree \" + str(irf) + \"\\n\"\n",
    "        if drift :\n",
    "            casetxt +=\"- with external drift\\n\"\n",
    "    if nvar > 1:\n",
    "        casetxt +=\"- number of covariables for co-kriging \" + str(nvar) + \"\\n\"\n",
    "        if flag_isotopic:\n",
    "            casetxt += \"- isotopic case\\n\"\n",
    "        else:\n",
    "            casetxt += \"- heterotopic case\\n\"\n",
    "    if measurement_error:\n",
    "        casetxt += \"- with measurement error\\n\"\n",
    "    else:\n",
    "        casetxt += \"- without measurement error\\n\"\n",
    "    if compute_vars:\n",
    "        casetxt += \"- no dual form\\n\"\n",
    "    else:\n",
    "        casetxt += \"- dual\\n\"\n",
    "    casetxt += \"- selection on Dbin \" + str(selDbin) + \"\\n\"\n",
    "    casetxt += \"- selection on Dbout \"+ str(selDbout) + \"\\n\"\n",
    "    casetxt += \"- number of data \" + str(ndat) + \"\\n\"\n",
    "    casetxt += \"- nx = [\"+str(nx[0]) +\",\" + str(nx[1]) + \"]\\n\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(casetxt)\n",
    "        \n",
    "    ##################################################\n",
    "    db,indF = createDbIn(ndat,nvar,percent,2,selDbin,measurement_error,ndrift,flag_isotopic,seed)\n",
    "    \n",
    "    target = gl.DbGrid.create(nx = nx)\n",
    "   \n",
    "    indOut = np.arange(target.getSampleNumber())\n",
    "    \n",
    "    if selDbout:\n",
    "        np.random.shuffle(indOut)\n",
    "        indOut = indOut[range(int(target.getSampleNumber()/2))]\n",
    "        indOut = np.sort(indOut)\n",
    "        sel = np.zeros(shape = target.getSampleNumber())\n",
    "        sel[indOut] = 1\n",
    "        target[\"sel\"] = sel\n",
    "        target.setLocator(\"sel\",gl.ELoc.SEL)\n",
    "                  \n",
    "    if irf is not None:\n",
    "        modeln.setDriftIRF(irf)\n",
    "    \n",
    "    if drift :\n",
    "        target[\"ff\"] = np.random.normal(size = target.getSampleNumber())\n",
    "        \n",
    "        target.setLocator(\"ff\",gl.ELoc.F)\n",
    "        modeln.addDrift(gl.DriftF(0))\n",
    "      \n",
    "    v = np.array([db[\"x0\"],db[\"x1\"]]).T\n",
    "    v0 = np.array([target[\"x1\"][indOut],target[\"x2\"][indOut]]).T\n",
    "    cov = cova(distance_matrix(v,v),modeln.getSillValues(0).toTL())[indF,:][:,indF]\n",
    "    c0  = cova(distance_matrix(v,v0),modeln.getSillValues(0).toTL())[indF,:]\n",
    "    #Creation of a db2 without selection to build the complete covariance matrix\n",
    "    db2 = db.clone()\n",
    "    db2.setLocator(\"sel\")\n",
    "    vect = gl.VectorDouble(nvar**2 * db2.getSampleNumber()**2)\n",
    "    \n",
    "    target2 = target.clone()\n",
    "    target2.setLocator(\"sel\")\n",
    "    covt = modeln.evalCovMatrixSymmetric(db).toTL()\n",
    "    c0gl = modeln.evalCovMatrix(db,target).toTL()\n",
    "    \n",
    "    if measurement_error:\n",
    "        err = db[\"err*\"].T.reshape(-1,)\n",
    "        np.fill_diagonal(cov,np.diag(cov)+err[indF])\n",
    "    \n",
    "    vect = gl.VectorDouble(nvar**2 * db2.getSampleNumber() * len(indOut))\n",
    "    \n",
    "    neigh = gl.NeighUnique()\n",
    "    \n",
    "    errcode = 0\n",
    "    diff = np.max(np.abs(cov-covt))\n",
    "    if diff > eps:\n",
    "        errcode = errcode + 1\n",
    "        print(f\"Difference in Cov  = {round(diff,4)}\")\n",
    "    diff = np.max(np.abs(c0-c0gl))\n",
    "    if diff > eps:\n",
    "        errcode = errcode + 1\n",
    "        print(f\"Difference in Cov0 = {round(diff,4)}\")\n",
    "    if irf is not None or drift:\n",
    "        driftd = np.kron(np.eye(nvar),modeln.getDrifts(db2, False))[:,indF]\n",
    "        driftt = np.kron(np.eye(nvar),modeln.getDrifts(target, True))\n",
    "        driftdgl = modeln.evalDriftMatrix(db).toTL().T\n",
    "        drifttgl = modeln.evalDriftMatrix(target).toTL().T\n",
    "        diff = np.max(np.abs(driftdgl-driftd))\n",
    "        if diff > eps:\n",
    "            errcode = errcode + 1\n",
    "            print(f\"Difference in X  = {round(diff,4)}\")\n",
    "        diff = np.max(np.abs(drifttgl-driftt))\n",
    "        if diff > eps:\n",
    "            errcode = errcode + 1\n",
    "            print(f\"Difference in X0 = {round(diff,4)}\")\n",
    "\n",
    "    return errcode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = [0.5,0.9,1.]\n",
    "nerr = 0\n",
    "ntest = 0\n",
    "for irf in [None,0,1]:\n",
    "  for drift in [False,True]:\n",
    "    for measurement_error in [True, False]:\n",
    "      for selDbin in [True, False]:\n",
    "        for selDbout in [True, False]:\n",
    "          for nx in [[5,5]]:\n",
    "            for nvar in [1,2,3]:\n",
    "              isolist = [True]\n",
    "              if nvar >1 :\n",
    "                isolist = [True,False]\n",
    "              for iso in isolist:\n",
    "                for cv in [False,True]:\n",
    "                  errcode = test_covmat(40,nx,nvar,percent,\n",
    "                                        model,cova,compute_vars=cv,\n",
    "                                        irf=irf,drift=drift,\n",
    "                                        measurement_error=measurement_error,\n",
    "                                        selDbin=selDbin,selDbout=selDbout,\n",
    "                                        flag_isotopic = False,\n",
    "                                        seed=1234,tol=1e-8,eps=1e-3,verbose=False)\n",
    "                  nerr = nerr + errcode\n",
    "                  ntest = ntest + 1\n",
    "                  \n",
    "print(ntest,\"test(s) have been performed with\", nerr, \"error(s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
